import os
import json
import sys
import time
import random
import msvcrt
from dotenv import load_dotenv
from openai import OpenAI
import faiss
from sentence_transformers import SentenceTransformer
from utils.preprocess import load_esconv
from utils.vector_index import encode_text
import threading
import queue
from tencentcloud.common import credential
from tencentcloud.tts.v20190823 import tts_client, models
from tencentcloud.common.profile.client_profile import ClientProfile
from tencentcloud.common.profile.http_profile import HttpProfile
from playsound import playsound
from sklearn.metrics.pairwise import cosine_similarity

#==========åˆæˆå£°éŸ³===========
'''def synthesize_speech(text, filename="output.mp3"):
    cred = credential.Credential(os.getenv("TENCENTCLOUD_SECRET_ID"), os.getenv("TENCENTCLOUD_SECRET_KEY"))
    httpProfile = HttpProfile()
    httpProfile.endpoint = "tts.tencentcloudapi.com"
    clientProfile = ClientProfile()
    clientProfile.httpProfile = httpProfile
    client = tts_client.TtsClient(cred, "ap-shanghai", clientProfile)

    req = models.TextToVoiceRequest()
    params = {
        "Text": text,
        "SessionId": str(random.randint(1000, 9999)),
        "ModelType": 2,
        "VoiceType": 101046,
        "Codec": "mp3",
        "SampleRate": 16000,
        "Speed": 0,
        "Volume": 5,
    }
    req.from_json_string(json.dumps(params))

    resp = client.TextToVoice(req)
    if resp.Audio:
        with open(filename, "wb") as f:
            f.write(resp.Audio)
        print(f"ğŸ”Š éŸ³é¢‘ä¿å­˜ä¸º {filename}")
    else:
        print("âš ï¸ æ²¡æœ‰åˆæˆéŸ³é¢‘å†…å®¹è¿”å›")
        
#=========å°è£…æ’­æ”¾åˆæˆéŸ³é¢‘============
def speak(text, filename="kimi_reply.mp3"):
    synthesize_speech(text, filename)
    playsound(filename)
    '''
#=========çº¿ç¨‹ç®¡ç†===========
os.chdir(os.path.dirname(sys.executable if getattr(sys, 'frozen', False) else __file__))
# ========== AUTO_CONTINUE_TEMPLATES ==========
AUTO_CONTINUE_TEMPLATES = [
    "æœ‰æ—¶å€™ä¸€æ—¶è¯´ä¸ä¸Šæ¥ä¹Ÿæ²¡å…³ç³»ï¼Œä½ è§‰å¾—è¿™ä¸ªè¯é¢˜è®©ä½ æƒ³åˆ°äº†ä»€ä¹ˆï¼Ÿ",
    "å¬èµ·æ¥è¿™é‡Œé¢å¥½åƒæœ‰äº›æ²¡è¯´å‡ºå£çš„ä¸œè¥¿ï¼Ÿä½ æ–¹ä¾¿è¯´è¯´çœ‹å—ï¼Ÿ",
    "æˆ‘ä»¬å¯ä»¥ä¸ç€æ€¥â€”â€”ä½ åˆšæ‰é‚£å¥è¯è®©æˆ‘æœ‰ç‚¹åœ¨æƒ³ï¼Œä½ è‡ªå·±æ˜¯æ€ä¹ˆç†è§£çš„ï¼Ÿ",
    "å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­æ²¿ç€åˆšæ‰é‚£ä¸ªè¯é¢˜èŠä¸‹å»ã€‚",
    "ä½ åˆšåˆšåœé¡¿äº†ä¸€ä¸‹ï¼Œæˆ‘çŒœä½ å¯èƒ½åœ¨æƒ³ä¸€äº›æ›´æ·±çš„äº‹æƒ…ï¼Ÿ",
]
# ========== åŠ è½½ç¯å¢ƒå˜é‡ ==========
load_dotenv("API.env")
MOONSHOT_API_KEY = os.getenv("MOONSHOT_API_KEY")
MOONSHOT_BASE_URL = os.getenv("MOONSHOT_BASE_URL")

# ========== åˆå§‹åŒ–å®¢æˆ·ç«¯ ==========
client = OpenAI(api_key=MOONSHOT_API_KEY, base_url=MOONSHOT_BASE_URL)

# ========== åŠ è½½ FAISS ä¸è¯­æ–™ ==========
print("ğŸ“¦ æ­£åœ¨åŠ è½½å‘é‡ç´¢å¼•ä¸è¯­æ–™åº“...")
index_path = "Data/esconv_faiss.index"
json_path = "Data/ESConv.json"
faiss_index = faiss.read_index(index_path)
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
corpus_pairs = load_esconv(json_path)

# ========== è¯»å– system prompt ==========
with open("system_prompt.txt", "r", encoding="utf-8") as f:
    system_prompt = {"role": "system", "content": f.read()}

chat_history = [system_prompt]
context_injected = False
top_k = 3

#=========è¯­ä¹‰ååŒæ‰“åˆ†==========
def evaluate_understanding(user_text, model_reply):
    try:
        # ç¿»è¯‘ä¸ºè‹±æ–‡
        def zh2en(text):
            response = client.chat.completions.create(
                model="moonshot-v1-8k",
                messages=[
                    {"role": "system", "content": "è¯·å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡ï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚"},
                    {"role": "user", "content": text}
                ],
                temperature=0.7,
            )
            return response.choices[0].message.content.strip()

        user_en = zh2en(user_text)
        reply_en = zh2en(model_reply)

        # ç¼–ç å‘é‡
        u_vec = encode_text(user_en, embedding_model)
        r_vec = encode_text(reply_en, embedding_model)

        # è®¡ç®—è·ç¦»ï¼ˆ1 - cosine simï¼‰
        sim = cosine_similarity(u_vec, r_vec)[0][0]
        nclid_score = 1 - sim  # è¶Šå°è¶Šå¥½

        # æ˜ å°„ä¸ºâ€œç†è§£æ„Ÿâ€åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        pu_score = max(0, 1 - nclid_score) * 5  # æ»¡åˆ†5åˆ†
        return pu_score

    except Exception as e:
        print("âš ï¸ ç†è§£è¯„åˆ†å¤±è´¥ï¼š", str(e))
        return 0.0

# ========== å›å¤å‡½æ•° ==========
def get_reply(prompt_messages, max_tokens=60):
    response = client.chat.completions.create(
        model="moonshot-v1-128k",
        messages=prompt_messages,
        temperature=0.8,
        timeout=20,
        response_format={"type": "json_object"},
        max_tokens=max_tokens
    )
    try:
        content = json.loads(response.choices[0].message.content)
        return content.get("text", "").strip()
    except json.JSONDecodeError:
        return response.choices[0].message.content.strip()

# ========== è¾“å…¥è¶…æ—¶å‡½æ•° ==========
def safe_input_with_timeout(prompt, timeout):
    """
    å®Œå…¨éé˜»å¡è¾“å…¥ï¼Œå¹¶å¼ºåˆ¶å¤„ç† ctrl-c ä¸ç©ºè¾“å…¥
    """
    input_queue = queue.Queue()

    def read_input(q):
        try:
            line = input()
            q.put(line.strip())
        except Exception:
            q.put("")

    print(prompt, end='', flush=True)
    thread = threading.Thread(target=read_input, args=(input_queue,), daemon=True)
    thread.start()
    thread.join(timeout)

    if not input_queue.empty():
        return input_queue.get()
    else:
        print()  # è‡ªåŠ¨æ¢è¡Œ
        return ""


# ========== è¯­æ–™æ³¨å…¥å‡½æ•° ==========
def inject_context(user_text, chat_history, context_injected_flag):
    try:
        translation_response = client.chat.completions.create(
            model="moonshot-v1-8k",
            messages=[
                {"role": "system", "content": "è¯·å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡ï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚"},
                {"role": "user", "content": user_text}
            ],
            temperature=0.7,
        )
        translated_query = translation_response.choices[0].message.content.strip()
        query_vector = encode_text(translated_query, embedding_model)
        D, I = faiss_index.search(query_vector, top_k)

        matched_contexts = []
        for idx in I[0]:
            if idx < len(corpus_pairs):
                entry = corpus_pairs[idx]
                matched_contexts.append(f"Q: {entry['question']}\nA: {entry['answer']}")

        if matched_contexts and not context_injected_flag:
            combined_context = "\n\n---\n\n".join(matched_contexts)
            chat_history.insert(1, {
                "role": "system",
                "content": f"ä»¥ä¸‹æ˜¯çŸ¥è¯†åº“èƒŒæ™¯ä¿¡æ¯ï¼Œè¯·åœ¨å›ç­”ä¸­å‚è€ƒä½†ä¸è¦é‡å¤å†…å®¹ï¼š\n\n{combined_context}"
            })
            return True
    except Exception as e:
        print("âŒ æ’å…¥è¯­æ–™å¤±è´¥ï¼š", str(e))
    return context_injected_flag


while True:
    try:
        user_input = safe_input_with_timeout("ä½ ï¼š", timeout=9999)

        if user_input.lower() in ["exit", "quit"]:
            break

        if not user_input:
            print("âš ï¸ æ²¡æœ‰å¬åˆ°ä½ çš„å›åº”ï¼Œè¦ä¸ä½ è¯´ç‚¹ä»€ä¹ˆï¼Ÿ")
            continue

        # === ä¸Šä¸‹æ–‡æ³¨å…¥ + å›å¤ ===
        context_injected = inject_context(user_input, chat_history, context_injected)
        chat_history.append({"role": "user", "content": user_input})

        first_sentence = get_reply(chat_history, max_tokens=200)
        print("\nğŸ¤– Kimiï¼š", first_sentence)
        score = evaluate_understanding(user_input, first_sentence)
        print(f"ğŸ“Š ç†è§£æ„Ÿè¯„åˆ†ï¼š{score:.2f}")
        #speak(first_sentence)
        chat_history.append({"role": "assistant", "content": first_sentence})

        # === æ²‰é»˜ç›‘å¬ + è‡ªåŠ¨ç»­è¯´ ===
        silent_rounds = 0
        while silent_rounds < 5:
            print("\n(ä½ å¯ä»¥æ¥ç€è¯´ï¼Œä¹Ÿå¯ä»¥æ²‰é»˜ 20 ç§’è®©æˆ‘ç»§ç»­)\n")
            follow_up = safe_input_with_timeout("ä½ ï¼š", timeout=20)
            last_input_empty = not follow_up.strip()

            if follow_up.strip():
                # ç”¨æˆ·ç»§ç»­è¯´
                context_injected = inject_context(follow_up, chat_history, context_injected)
                chat_history.append({"role": "user", "content": follow_up})

                follow_reply = get_reply(chat_history, max_tokens=200)
                print("\nğŸ¤– Kimiï¼š", follow_reply)
                #speak(follow_reply)
                score = evaluate_understanding(user_input, follow_reply)
                print(f"ğŸ“Š ç†è§£æ„Ÿè¯„åˆ†ï¼š{score:.2f}")
                chat_history.append({"role": "assistant", "content": follow_reply})
                silent_rounds = 0  # é‡ç½®æ²‰é»˜è®¡æ•°å™¨
            else:
                # ç”¨æˆ·æ²‰é»˜ï¼ŒAI ä¸»åŠ¨ç»§ç»­è¯´
                auto_prompt = random.choice(AUTO_CONTINUE_TEMPLATES)
                chat_history.append({
                    "role": "system",
                    "content": f"ç”¨æˆ·æ²‰é»˜äº†ï¼Œè¯·ä½ ä»¥æ¸©æŸ”æœ‹å‹çš„è¯­æ°”ç»§ç»­è¯´ä¸€äº›è¯ï¼Œå‚è€ƒè¿™æ¡æç¤ºï¼š{auto_prompt}"
                })

                continuation = get_reply(chat_history, max_tokens=200)
                if continuation.strip():
                    print("ğŸ¤– Kimiï¼ˆç»§ç»­ï¼‰ï¼š", continuation)
                    #speak(continuation)
                    score = evaluate_understanding(user_input, continuation)
                    print(f"ğŸ“Š ç†è§£æ„Ÿè¯„åˆ†ï¼š{score:.2f}")
                    chat_history.append({"role": "assistant", "content": continuation})
                silent_rounds += 1

    except Exception as e:
        print("âŒ å‡ºç°é”™è¯¯ï¼š", str(e))