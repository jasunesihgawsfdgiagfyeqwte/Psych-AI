
import os
import json
import time
import random
import csv
import threading
from flask import Flask, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
from openai import OpenAI
import faiss
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from utils.preprocess import load_esconv
from utils.vector_index import encode_text

app = Flask(__name__)
CORS(app)

#=====è¡¨å•è®°å½•======
log_fields = ["timestamp", "user_id", "user_input", "model_reply", "score"]

def write_log(timestamp, user_id, user_input, model_reply, score):
    with open("chat_logs.csv", "a", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=log_fields, quoting=csv.QUOTE_ALL)
        if f.tell() == 0:
            writer.writeheader()
        writer.writerow({
            "timestamp": timestamp,
            "user_id": user_id,
            "user_input": user_input,
            "model_reply": model_reply,
            "score": score
        })

# === åŠ è½½ç¯å¢ƒå˜é‡å’Œæ¨¡å‹ ===
load_dotenv("API.env")
MOONSHOT_API_KEY = os.getenv("MOONSHOT_API_KEY")
MOONSHOT_BASE_URL = os.getenv("MOONSHOT_BASE_URL")
client = OpenAI(api_key=MOONSHOT_API_KEY, base_url=MOONSHOT_BASE_URL)

print("ğŸ“¦ æ­£åœ¨åŠ è½½å‘é‡ç´¢å¼•ä¸è¯­æ–™åº“...")
index_path = "Data/esconv_faiss.index"
json_path = "Data/ESConv.json"
faiss_index = faiss.read_index(index_path)
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
corpus_pairs = load_esconv(json_path)

# === åˆå§‹åŒ–é…ç½® ===
top_k = 3
with open("system_prompt.txt", "r", encoding="utf-8") as f:
    system_prompt = {"role": "system", "content": f.read()}

AUTO_CONTINUE_TEMPLATES = [
    "æœ‰æ—¶å€™ä¸€æ—¶è¯´ä¸ä¸Šæ¥ä¹Ÿæ²¡å…³ç³»ï¼Œä½ è§‰å¾—è¿™ä¸ªè¯é¢˜è®©ä½ æƒ³åˆ°äº†ä»€ä¹ˆï¼Ÿ",
    "å¬èµ·æ¥è¿™é‡Œé¢å¥½åƒæœ‰äº›æ²¡è¯´å‡ºå£çš„ä¸œè¥¿ï¼Ÿä½ æ–¹ä¾¿è¯´è¯´çœ‹å—ï¼Ÿ",
    "æˆ‘ä»¬å¯ä»¥ä¸ç€æ€¥â€”â€”ä½ åˆšæ‰é‚£å¥è¯è®©æˆ‘æœ‰ç‚¹åœ¨æƒ³ï¼Œä½ è‡ªå·±æ˜¯æ€ä¹ˆç†è§£çš„ï¼Ÿ",
    "å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­æ²¿ç€åˆšæ‰é‚£ä¸ªè¯é¢˜èŠä¸‹å»ã€‚",
    "ä½ åˆšåˆšåœé¡¿äº†ä¸€ä¸‹ï¼Œæˆ‘çŒœä½ å¯èƒ½åœ¨æƒ³ä¸€äº›æ›´æ·±çš„äº‹æƒ…ï¼Ÿ",
]

# === å¤šç”¨æˆ·è®°å¿†ç³»ç»Ÿ ===
session_memory = {}

def encode_zh(text):
    res = client.chat.completions.create(
        model="moonshot-v1-8k",
        messages=[
            {"role": "system", "content": "è¯·å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡ï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚"},
            {"role": "user", "content": text}
        ],
        temperature=0.7,
    )
    return res.choices[0].message.content.strip()

def evaluate_understanding(user_text, model_reply):
    try:
        user_en = encode_zh(user_text)
        reply_en = encode_zh(model_reply)
        u_vec = encode_text(user_en, embedding_model)
        r_vec = encode_text(reply_en, embedding_model)
        sim = cosine_similarity(u_vec, r_vec)[0][0]
        return max(0, 1 - (1 - sim)) * 5
    except Exception as e:
        print("âš ï¸ ç†è§£è¯„åˆ†å¤±è´¥ï¼š", str(e))
        return 0.0

def inject_context(user_text, history):
    try:
        translated = encode_zh(user_text)
        vec = encode_text(translated, embedding_model)
        D, I = faiss_index.search(vec, top_k)
        matched = [f"Q: {corpus_pairs[i]['question']}\nA: {corpus_pairs[i]['answer']}" for i in I[0] if i < len(corpus_pairs)]
        if matched:
            context = "\n\n---\n\n".join(matched)
            history.insert(1, {"role": "system", "content": f"ä»¥ä¸‹æ˜¯çŸ¥è¯†åº“èƒŒæ™¯ä¿¡æ¯ï¼Œè¯·åœ¨å›ç­”ä¸­å‚è€ƒä½†ä¸è¦é‡å¤å†…å®¹ï¼š\n\n{context}"})
    except Exception as e:
        print("âŒ æ’å…¥è¯­æ–™å¤±è´¥ï¼š", str(e))

def get_reply(history, max_tokens=200):
    res = client.chat.completions.create(
        model="moonshot-v1-128k",
        messages=history,
        temperature=0.8,
        timeout=20,
        response_format={"type": "json_object"},
        max_tokens=max_tokens
    )
    try:
        content = json.loads(res.choices[0].message.content)
        return content.get("text", "").strip()
    except:
        return res.choices[0].message.content.strip()

@app.route("/chat", methods=["POST"])
def chat():
    data = request.json
    user_input = data.get("text", "").strip()
    user_id = data.get("user_id", "anonymous")

    if not user_input:
        return jsonify({"text": "âš ï¸ è¯·è¾“å…¥å†…å®¹"})

    # åˆå§‹åŒ–ç”¨æˆ·
    if user_id not in session_memory:
        session_memory[user_id] = {
            "history": [system_prompt],
            "last_active": time.time()
        }

    session = session_memory[user_id]
    session["last_active"] = time.time()
    history = session["history"]

    inject_context(user_input, history)
    history.append({"role": "user", "content": user_input})
    reply = f"[reply] {get_reply(history)}"
    history.append({"role": "assistant", "content": reply})
    score = evaluate_understanding(user_input, reply)

    print(f"[{user_id}] ğŸ‘¤ {user_input}")
    print(f"[{user_id}] ğŸ¤– {reply} | ğŸ“Š Score: {score:.2f}")

    write_log(time.time(), user_id, user_input, reply, score)

    return jsonify({"text": reply})

#========è‡ªåŠ¨ç»­è¯´æ£€æµ‹===========
@app.route("/check_update", methods=["POST"])
def check_update():
    data = request.json
    user_id = data.get("user_id", "anonymous")

    if user_id not in session_memory:
        return jsonify({"update": False})

    session = session_memory[user_id]
    pending_reply = session.get("pending_auto_reply", None)

    if pending_reply:
        session["pending_auto_reply"] = None  # æ¸…é™¤å·²è¯»
        return jsonify({"update": True, "text": pending_reply})
    else:
        return jsonify({"update": False})

# === è‡ªåŠ¨ç»­è¯´çº¿ç¨‹ ===
def auto_continue_check():
    while True:
        time.sleep(10)
        now = time.time()
        for uid, session in session_memory.items():
            last_time = session.get("last_active", now)
            if now - last_time > 30:
                history = session["history"]
                prompt = random.choice(AUTO_CONTINUE_TEMPLATES)
                history.append({
                    "role": "system",
                    "content": f"ç”¨æˆ·æ²‰é»˜äº†ï¼Œè¯·ä½ ä»¥æ¸©æŸ”æœ‹å‹çš„è¯­æ°”ç»§ç»­è¯´ä¸€äº›è¯ï¼Œå‚è€ƒè¿™æ¡æç¤ºï¼š{prompt}"
                })
                reply = get_reply(history)
                history.append({"role": "assistant", "content": reply})
                session["pending_auto_reply"] = reply
                write_log(time.time(), uid, "[auto_continue]", reply, "")

                session["last_active"] = now
                print(f"[{uid}] ğŸ¤– è‡ªåŠ¨ç»­è¯´ï¼š{reply}")

if __name__ == "__main__":
    threading.Thread(target=auto_continue_check, daemon=True).start()
    app.run(port=5005)
